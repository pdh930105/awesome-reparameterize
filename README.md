# Awesome-Reparameterize <br>[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
### This page's style is based on **[Awesome-Efficient-LLM](https://github.com/horseee/Awesome-Efficient-LLM/tree/main)**. Thank you for providing a great format. 

A curated list for Reparameterize:
  - [CNN](#cnn)
  - [Transformer/ViT](#Transformer/ViT)
  - [Quantization for Reparameterize](#Quantization-for-Reparameterize)
  - [Specific Domain](#specific-domain)
  - [Others](#Others)


#### ðŸš€ Future Works
[ ] **Sorting papers by year, conference**

[ ] **Upload an image of the main idea of the paper**

[] **Support journal format (TPAMI, TNNLS, TIP, etc)**
#### ðŸ’® Contributing

If you'd like to include other reparameterization paper, or need to update any details such as conference information or code URLs, please feel free to submit a pull request. You can generate the required markdown format for each paper by filling in the information in `generate_item.py` and execute `python generate_item.py`. (or using `genetarte_markdown.ipynb`)  We warmly appreciate your contributions to this list. Alternatively, you can email me with the links to your paper and code, and I would add your paper to the list at my earliest convenience. 


## CNN
| Title & Authors | Introduction | Links |
|:----|  :----: | :---:|
|[![Star](https://img.shields.io/github/stars/DingXiaoH/ACNet.svg?style=social&label=Star)](https://github.com/DingXiaoH/ACNet)[![Publish](https://img.shields.io/badge/Conference-ICCV'19-blue)]()<br>[ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_ACNet_Strengthening_the_Kernel_Skeletons_for_Powerful_CNN_via_Asymmetric_ICCV_2019_paper.pdf) <br> Xiaohan. Ding, Yuchen. Guo, Guiguang. Ding, Jungong. Han,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/DingXiaoH/ACNet) <br> [Paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_ACNet_Strengthening_the_Kernel_Skeletons_for_Powerful_CNN_via_Asymmetric_ICCV_2019_paper.pdf)|
|[![Star](https://img.shields.io/github/stars/DingXiaoH/DiverseBranchBlock.svg?style=social&label=Star)](https://github.com/DingXiaoH/DiverseBranchBlock)[![Publish](https://img.shields.io/badge/Conference-CVPR'21-blue)]()<br>[Diverse branch block: Building a convolution as an inception-like unit](https://arxiv.org/abs/2103.13425) <br> Xiaohan. Ding, Xiangyu. Zhang, Jungong. Han, Guiguang. Ding,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/DingXiaoH/DiverseBranchBlock) <br> [Paper](https://arxiv.org/abs/2103.13425)|
|[![Star](https://img.shields.io/github/stars/DingXiaoH/RepVGG.svg?style=social&label=Star)](https://github.com/DingXiaoH/RepVGG)[![Publish](https://img.shields.io/badge/Conference-CVPR'21-blue)]()<br>[Repvgg: Making vgg-style convnets great again](https://openaccess.thecvf.com/content/CVPR2021/papers/Ding_RepVGG_Making_VGG-Style_ConvNets_Great_Again_CVPR_2021_paper.pdf) <br> Xiaohan. Ding, Xiangyu. Zhang, Ningning. Ma, Jungong. Han, Guiguang. Ding, Jian. Sun,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/DingXiaoH/RepVGG) <br> [Paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Ding_RepVGG_Making_VGG-Style_ConvNets_Great_Again_CVPR_2021_paper.pdf)|
|[![Star](https://img.shields.io/github/stars/apple/ml-mobileone.svg?style=social&label=Star)](https://github.com/apple/ml-mobileone)[![Publish](https://img.shields.io/badge/Conference-CVPR'23-blue)]()<br>[MobileOne: An Improved One Millisecond Mobile Backbone](https://openaccess.thecvf.com/content/CVPR2023/papers/Vasu_MobileOne_An_Improved_One_Millisecond_Mobile_Backbone_CVPR_2023_paper.pdf) <br> Pavan. Vasu, James. Gabriel, Jeff. Zhu, Oncel. Tuzel, Anurag. Ranjan,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/apple/ml-mobileone) <br> [Paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Vasu_MobileOne_An_Improved_One_Millisecond_Mobile_Backbone_CVPR_2023_paper.pdf)|
|[![Star](https://img.shields.io/github/stars/MegEngine/RepLKNet.svg?style=social&label=Star)](https://github.com/MegEngine/RepLKNet)[![Publish](https://img.shields.io/badge/Conference-CVPR'22-blue)]()<br>[Scaling up your kernels to 31x31: Revisiting large kernel design in cnns](https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.pdf) <br> Xiaohan. Ding, Xiangyu. Zhang, Jungong. Han, Guiguang. Ding,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/MegEngine/RepLKNet) <br> [Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.pdf)|
|[![Star](https://img.shields.io/github/stars/AILab-CVC/UniRepLKNet.svg?style=social&label=Star)](https://github.com/AILab-CVC/UniRepLKNet)<br>[UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition](https://arxiv.org/pdf/2311.15599.pdf) <br> Xiaohan. Ding, Yiyuan. Zhang, Yixiao. Ge, Sijie. Zhao, Lin. Song, Xiangyu. Yue, Ying. Shan,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/AILab-CVC/UniRepLKNet) <br> [Paper](https://arxiv.org/pdf/2311.15599.pdf)|
|[![Star](https://img.shields.io/github/stars/VITA-Group/SLaK.svg?style=social&label=Star)](https://github.com/VITA-Group/SLaK)[![Publish](https://img.shields.io/badge/Conference-ICLR'23-blue)]()<br>[More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity](https://arxiv.org/pdf/2207.03620.pdf) <br> Shiwei. Liu, Tianlong. Chen, Xiaohan. Chen, Xuxi. Chen, Qiao. Xiao, Boqian. Wu, Tommi. K{"a}rkk{"a}inen, Mykola. Pechenizkiy, Decebal. Mocanu, Zhangyang. Wang,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/VITA-Group/SLaK) <br> [Paper](https://arxiv.org/pdf/2207.03620.pdf)|
|[![Star](https://img.shields.io/github/stars/ChengpengChen/RepGhost.svg?style=social&label=Star)](https://github.com/ChengpengChen/RepGhost)<br>[Repghost: a hardware-efficient ghost module via re-parameterization](https://arxiv.org/pdf/2211.06088.pdf) <br> Chengpeng. Chen, Zichao. Guo, Haien. Zeng, Pengfei. Xiong, Jian. Dong,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/ChengpengChen/RepGhost) <br> [Paper](https://arxiv.org/pdf/2211.06088.pdf)|
|<br>[RepRCNN: A Structural Reparameterisation Convolutional Neural Network Object Detection Algorithm Based on Branch Matching](https://www.mdpi.com/2079-9292/12/19/4180/pdf?version=1696836779) <br> Xudong. Li, Xinyao. Lv, Linghui. Sun, Jingzhi. Zhang, Ruoming. Lan,  |<img width="1002" alt="image" src="figures/.png"> |[Paper](https://www.mdpi.com/2079-9292/12/19/4180/pdf?version=1696836779)|
|[![Star](https://img.shields.io/github/stars/facebookresearch/DepthShrinker.svg?style=social&label=Star)](https://github.com/facebookresearch/DepthShrinker)[![Publish](https://img.shields.io/badge/Conference-ICML'22-blue)]()<br>[DepthShrinker: a new compression paradigm towards boosting real-hardware efficiency of compact neural networks](https://proceedings.mlr.press/v162/fu22c/fu22c.pdf) <br> Yonggan. Fu, Haichuan. Yang, Jiayi. Yuan, Meng. Li, Cheng. Wan, Raghuraman. Krishnamoorthi, Vikas. Chandra, Yingyan. Lin,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/facebookresearch/DepthShrinker) <br> [Paper](https://proceedings.mlr.press/v162/fu22c/fu22c.pdf)|
|[![Publish](https://img.shields.io/badge/Conference-DAC'23-blue)]()<br>[NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders of Deep Giants](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247827) <br> Zhongzhi. Yu, Yonggan. Fu, Jiayi. Yuan, Haoran. You, Yingyan. Lin,  |<img width="1002" alt="image" src="figures/.png"> |[Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247827)|
|[![Star](https://img.shields.io/github/stars/hunto/DyRep.svg?style=social&label=Star)](https://github.com/hunto/DyRep)[![Publish](https://img.shields.io/badge/Conference-CVPR'22-blue)]()<br>[Dyrep: bootstrapping training with dynamic re-parameterization](https://openaccess.thecvf.com/content/CVPR2022/papers/Huang_DyRep_Bootstrapping_Training_With_Dynamic_Re-Parameterization_CVPR_2022_paper.pdf) <br> Tao. Huang, Shan. You, Bohan. Zhang, Yuxuan. Du, Fei. Wang, Chen. Qian, Chang. Xu,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/hunto/DyRep) <br> [Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Huang_DyRep_Bootstrapping_Training_With_Dynamic_Re-Parameterization_CVPR_2022_paper.pdf)|
|<br>[ASR: Attention-alike Structural Re-parameterization](https://arxiv.org/pdf/2304.06345.pdf) <br> Shanshan. Zhong, Zhongzhan. Huang, Wushao. Wen, Jinghui. Qin, Liang. Lin,  |<img width="1002" alt="image" src="figures/.png"> |[Paper](https://arxiv.org/pdf/2304.06345.pdf)|


## Transformer/ViT
| Title & Authors | Introduction | Links |
|:----|  :----: | :---:|
|[![Star](https://img.shields.io/github/stars/zkkli/RepQ-ViT.svg?style=social&label=Star)](https://github.com/zkkli/RepQ-ViT)[![Publish](https://img.shields.io/badge/Conference-ICCV'23-blue)]()<br>[Repq-vit: Scale reparameterization for post-training quantization of vision transformers](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_RepQ-ViT_Scale_Reparameterization_for_Post-Training_Quantization_of_Vision_Transformers_ICCV_2023_paper.pdf) <br> Zhikai. Li, Junrui. Xiao, Lianwei. Yang, Qingyi. Gu,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/zkkli/RepQ-ViT) <br> [Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_RepQ-ViT_Scale_Reparameterization_for_Post-Training_Quantization_of_Vision_Transformers_ICCV_2023_paper.pdf)|
|[![Star](https://img.shields.io/github/stars/Correr-Zhou/RepMode.svg?style=social&label=Star)](https://github.com/Correr-Zhou/RepMode)[![Publish](https://img.shields.io/badge/Conference-ICCV'23-blue)]()<br>[FastViT: A Fast Hybrid Vision Transformer Using Structural Reparameterization](https://arxiv.org/abs/2303.14189) <br> Pavan. Vasu, James. Gabriel, Jeff. Zhu, Oncel. Tuzel, Anurag. Ranjan,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/Correr-Zhou/RepMode) <br> [Paper](https://arxiv.org/abs/2303.14189)|
|[![Star](https://img.shields.io/github/stars/THU-MIG/RepViT.svg?style=social&label=Star)](https://github.com/THU-MIG/RepViT)<br>[Repvit: Revisiting mobile cnn from vit perspective](https://arxiv.org/abs/2307.09283) <br> Ao. Wang, Hui. Chen, Zijia. Lin, Hengjun. Pu, Guiguang. Ding,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/THU-MIG/RepViT) <br> [Paper](https://arxiv.org/abs/2307.09283)|
|[![Star](https://img.shields.io/github/stars/tany0699/FMViT.svg?style=social&label=Star)](https://github.com/tany0699/FMViT)<br>[FMViT: A multiple-frequency mixing Vision Transformer](https://arxiv.org/abs/2311.05707.pdf) <br> Wei. Tan, Yifeng. Geng, Xuansong. Xie,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/tany0699/FMViT) <br> [Paper](https://arxiv.org/abs/2311.05707.pdf)|


## Quantization-for-Reparameterize
| Title & Authors | Introduction | Links |
|:----|  :----: | :---:|
|[![Publish](https://img.shields.io/badge/Conference-AAAI'24-blue)]()<br>[Make RepVGG Greater Again: A Quantization-aware Approach](https://arxiv.org/pdf/2212.01593.pdf) <br> Xiangxiang. Chu, Liang. Li, Bo. Zhang,  |<img width="1002" alt="image" src="figures/.png"> |[Paper](https://arxiv.org/pdf/2212.01593.pdf)|
|[![Publish](https://img.shields.io/badge/Conference-BMVC'23-blue)]()<br>[RepQ: Generalizing Quantization-Aware Training for Re-Parametrized Architectures](https://papers.bmvc2023.org/0311.pdf) <br> Anastasiia. Prutianova, Alexey. Zaytsev, Chung-Kuei. Lee, Fengyu. Sun, Ivan. Koryakovskiy,  |<img width="1002" alt="image" src="figures/.png"> |[Paper](https://papers.bmvc2023.org/0311.pdf)|
|[![Star](https://img.shields.io/github/stars/NeonHo/Coarse-Fine-Weight-Split.svg?style=social&label=Star)](https://github.com/NeonHo/Coarse-Fine-Weight-Split)<br>[Post-Training Quantization for Re-parameterization via Coarse \& Fine Weight Splitting](https://arxiv.org/pdf/2312.10588.pdf) <br> Dawei. Yang, Ning. He, Xing. Hu, Zhihang. Yuan, Jiangyong. Yu, Chen. Xu, Zhe. Jiang,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/NeonHo/Coarse-Fine-Weight-Split) <br> [Paper](https://arxiv.org/pdf/2312.10588.pdf)|

## Specific-domain (Object Detection, Segmantation, etc...)
| Title & Authors | Introduction | Links |
|:----|  :----: | :---:|
|[![Star](https://img.shields.io/github/stars/Correr-Zhou/RepMode.svg?style=social&label=Star)](https://github.com/Correr-Zhou/RepMode)[![Publish](https://img.shields.io/badge/Conference-CVPR'23-blue)]()<br>[RepMode: Learning to Re-parameterize Diverse Experts for Subcellular Structure Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_RepMode_Learning_to_Re-Parameterize_Diverse_Experts_for_Subcellular_Structure_Prediction_CVPR_2023_paper.pdf) <br> Donghao. Zhou, Chunbin. Gu, Junde. Xu, Furui. Liu, Qiong. Wang, Guangyong. Chen, Pheng-Ann. Heng,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/Correr-Zhou/RepMode) <br> [Paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_RepMode_Learning_to_Re-Parameterize_Diverse_Experts_for_Subcellular_Structure_Prediction_CVPR_2023_paper.pdf)|
|[![Star](https://img.shields.io/github/stars/THU-MIG/RepViT.svg?style=social&label=Star)](https://github.com/THU-MIG/RepViT)<br>[RepViT-SAM: Towards Real-Time Segmenting Anything](https://arxiv.org/abs/2312.05760) <br> Ao. Wang, Hui. Chen, Zijia. Lin, Jungong. Han, Guiguang. Ding,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/THU-MIG/RepViT) <br> [Paper](https://arxiv.org/abs/2312.05760)|
|[![Star](https://img.shields.io/github/stars/thohemp/6DRepNet.svg?style=social&label=Star)](https://github.com/thohemp/6DRepNet)<br>[6d rotation representation for unconstrained head pose estimation](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9897219) <br> Thorsten. Hempel, Ahmed. Abdelrahman, Ayoub. Al-Hamadi,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/thohemp/6DRepNet) <br> [Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9897219)|
|[![Star](https://img.shields.io/github/stars/meituan/YOLOv6.svg?style=social&label=Star)](https://github.com/meituan/YOLOv6)<br>[YOLOv6: A single-stage object detection framework for industrial applications](https://arxiv.org/pdf/2209.02976.pdf) <br> Chuyi. Li, Lulu. Li, Hongliang. Jiang, Kaiheng. Weng, Yifei. Geng, Liang. Li, Zaidan. Ke, Qingyuan. Li, Meng. Cheng, Weiqiang. Nie, others,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/meituan/YOLOv6) <br> [Paper](https://arxiv.org/pdf/2209.02976.pdf)|
|[![Star](https://img.shields.io/github/stars/WongKinYiu/yolov7.svg?style=social&label=Star)](https://github.com/WongKinYiu/yolov7)<br>[YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors](https://arxiv.org/abs/2207.02696) <br> Chien-Yao. Wang, Alexey. Bochkovskiy, Hong-Yuan. Liao,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/WongKinYiu/yolov7) <br> [Paper](https://arxiv.org/abs/2207.02696)|
|[![Star](https://img.shields.io/github/stars/ultralytics/ultralytics.svg?style=social&label=Star)](https://github.com/ultralytics/ultralytics)<br>[Ultralytics YOLO (YOLO v8)]() <br> Glenn. Jocher, Ayush. Chaurasia, Jing. Qiu,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/ultralytics/ultralytics) <br> [Docs](https://docs.ultralytics.com/)|


## Others
| Title & Authors | Introduction | Links |
|:----|  :----: | :---:|
|[![Star](https://img.shields.io/github/stars/DingXiaoH/RepOptimizers.svg?style=social&label=Star)](https://github.com/DingXiaoH/RepOptimizers)[![Publish](https://img.shields.io/badge/Conference-ICLR'23-blue)]()<br>[Re-parameterizing your optimizers rather than architectures](https://arxiv.org/abs/2205.15242) <br> X. Ding, H. Chen, X. Zhang, K. Huang, J. Han, G. Ding,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/DingXiaoH/RepOptimizers) <br> [Paper](https://arxiv.org/abs/2205.15242)|
|[![Star](https://img.shields.io/github/stars/DingXiaoH/RepMLP.svg?style=social&label=Star)](https://github.com/DingXiaoH/RepMLP)[![Publish](https://img.shields.io/badge/Conference-CVPR'22-blue)]()<br>[Repmlpnet: Hierarchical vision mlp with re-parameterized locality](https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_RepMLPNet_Hierarchical_Vision_MLP_With_Re-Parameterized_Locality_CVPR_2022_paper.pdf) <br> Xiaohan. Ding, Honghao. Chen, Xiangyu. Zhang, Jungong. Han, Guiguang. Ding,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/DingXiaoH/RepMLP) <br> [Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_RepMLPNet_Hierarchical_Vision_MLP_With_Re-Parameterized_Locality_CVPR_2022_paper.pdf)|
|[![Star](https://img.shields.io/github/stars/zyxxmu/SpRe.svg?style=social&label=Star)](https://github.com/zyxxmu/SpRe)<br>[Spatial Re-parameterization for N: M Sparsity](https://arxiv.org/abs/2306.05612.pdf) <br> Yuxin. Zhang, Mingbao. Lin, Yunshan. Zhong, Mengzhao. Chen, Fei. Chao, Rongrong. Ji,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/zyxxmu/SpRe) <br> [Paper](https://arxiv.org/abs/2306.05612.pdf)|
|[![Star](https://img.shields.io/github/stars/Ascend-Research/Reparameterization.svg?style=social&label=Star)](https://github.com/Ascend-Research/Reparameterization)[![Publish](https://img.shields.io/badge/Conference-ICLR'23-blue)]()<br>[Reparameterization through Spatial Gradient Scaling](https://arxiv.org/pdf/2303.02733.pdf) <br> Alexander. Detkov, Mohammad. Salameh, Muhammad. Fetrat, Jialin. Zhang, Robin. Luwei, SHANGLING. JUI, Di. Niu,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/Ascend-Research/Reparameterization) <br> [Paper](https://arxiv.org/pdf/2303.02733.pdf)|
|[![Star](https://img.shields.io/github/stars/JUGGHM/OREPA_CVPR2022.svg?style=social&label=Star)](https://github.com/JUGGHM/OREPA_CVPR2022)[![Publish](https://img.shields.io/badge/Conference-CVPR'22-blue)]()<br>[Online convolutional re-parameterization](https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_Online_Convolutional_Re-Parameterization_CVPR_2022_paper.pdf) <br> Mu. Hu, Junyi. Feng, Jiashen. Hua, Baisheng. Lai, Jianqiang. Huang, Xiaojin. Gong, Xian-Sheng. Hua,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/JUGGHM/OREPA_CVPR2022) <br> [Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_Online_Convolutional_Re-Parameterization_CVPR_2022_paper.pdf)|
|[![Star](https://img.shields.io/github/stars/DingXiaoH/ResRep.svg?style=social&label=Star)](https://github.com/DingXiaoH/ResRep)[![Publish](https://img.shields.io/badge/Conference-ICCV'21-blue)]()<br>[Resrep: Lossless cnn pruning via decoupling remembering and forgetting](https://openaccess.thecvf.com/content/ICCV2021/papers/Ding_ResRep_Lossless_CNN_Pruning_via_Decoupling_Remembering_and_Forgetting_ICCV_2021_paper.pdf) <br> Xiaohan. Ding, Tianxiang. Hao, Jianchao. Tan, Ji. Liu, Jungong. Han, Yuchen. Guo, Guiguang. Ding,  |<img width="1002" alt="image" src="figures/.png"> |[Github](https://github.com/DingXiaoH/ResRep) <br> [Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Ding_ResRep_Lossless_CNN_Pruning_via_Decoupling_Remembering_and_Forgetting_ICCV_2021_paper.pdf)|